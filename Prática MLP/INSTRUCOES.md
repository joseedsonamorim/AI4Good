# ğŸ“‹ InstruÃ§Ãµes de Uso - MLP from Scratch

## ğŸš€ ExecuÃ§Ã£o RÃ¡pida

### OpÃ§Ã£o 1: Script AutomÃ¡tico (Recomendado)
```bash
./run.sh
```

### OpÃ§Ã£o 2: Manual
```bash
# Criar ambiente virtual
python3 -m venv mlp_env

# Instalar dependÃªncias
mlp_env/bin/pip install numpy matplotlib pandas seaborn

# Executar
mlp_env/bin/python mlp_from_scratch.py
```

## ğŸ“ Estrutura do Projeto

```
Criando uma MLP/
â”œâ”€â”€ mlp_from_scratch.py      # ImplementaÃ§Ã£o principal da MLP
â”œâ”€â”€ run.sh                   # Script de execuÃ§Ã£o automÃ¡tica
â”œâ”€â”€ requirements.txt         # DependÃªncias Python
â”œâ”€â”€ README.md               # DocumentaÃ§Ã£o principal
â”œâ”€â”€ INSTRUCOES.md           # Este arquivo
â””â”€â”€ mlp_env/                # Ambiente virtual Python
```

## ğŸ¯ ExecuÃ§Ã£o

### Treinamento Completo
- **Arquivo**: `mlp_from_scratch.py`
- **Dataset**: Heart Disease real do Kaggle (1,025 amostras)
- **Tempo**: ~2-5 minutos
- **Objetivo**: Treinamento completo com visualizaÃ§Ã£o em tempo real

## ğŸ”§ PersonalizaÃ§Ã£o

### Modificar Arquitetura
Edite diretamente no `mlp_from_scratch.py`:
```python
# Na funÃ§Ã£o main(), linha ~450
layers = [13, 16, 8, 1]  # Sua arquitetura
mlp = MLP(layers, learning_rate=0.05)  # Sua taxa de aprendizado
```

### Modificar ParÃ¢metros de Treinamento
```python
# Na funÃ§Ã£o main(), linha ~460
history = mlp.train(X_train, y_train, epochs=1000, visualize=True)
```

## ğŸ“Š Interpretando os Resultados

### MÃ©tricas Importantes
- **Loss**: Erro quadrÃ¡tico mÃ©dio (menor = melhor)
- **Accuracy**: Taxa de acerto (maior = melhor)
- **ConfianÃ§a**: Probabilidade da prediÃ§Ã£o (0-1)

### VisualizaÃ§Ãµes (Com ExplicaÃ§Ãµes Detalhadas)
1. **ğŸ—ï¸ Arquitetura**: Grafo com neurÃ´nios e conexÃµes coloridas + legendas explicativas
2. **ğŸ“ˆ Progresso**: Curvas de loss/accuracy + valores atuais em tempo real
3. **âš–ï¸ Pesos**: Histograma separando pesos positivos/negativos + estatÃ­sticas
4. **ğŸ”¥ AtivaÃ§Ãµes**: Barras coloridas por intensidade + descriÃ§Ãµes das camadas

**ğŸ“š Painel Explicativo**: Texto na parte inferior explica todo o processo de treinamento

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro: "ModuleNotFoundError"
```bash
# Reinstalar dependÃªncias
mlp_env/bin/pip install --upgrade numpy matplotlib pandas seaborn
```

### Erro: "Permission denied" no run.sh
```bash
chmod +x run.sh
```

### VisualizaÃ§Ã£o nÃ£o aparece
- Verifique se estÃ¡ usando ambiente grÃ¡fico
- Tente executar sem visualizaÃ§Ã£o: `visualize=False`

### Performance lenta
- Reduza nÃºmero de Ã©pocas
- Aumente batch_size
- Desative visualizaÃ§Ã£o

## ğŸ“ Conceitos Demonstrados

### Forward Propagation
1. Valores de entrada â†’ primeira camada
2. Soma ponderada + bias
3. AplicaÃ§Ã£o de funÃ§Ã£o de ativaÃ§Ã£o
4. PropagaÃ§Ã£o para prÃ³xima camada

### Backward Propagation
1. CÃ¡lculo do erro na saÃ­da
2. PropagaÃ§Ã£o reversa dos gradientes
3. AtualizaÃ§Ã£o de pesos e bias
4. OtimizaÃ§Ã£o por gradiente descendente

### Estrutura de Grafo
- **NÃ³s**: Representam neurÃ´nios
- **ConexÃµes**: Representam pesos
- **Camadas**: Agrupamento de nÃ³s

## ğŸ”¬ Experimentos Sugeridos

### 1. Teste de Arquiteturas
```python
# Teste diferentes tamanhos na funÃ§Ã£o main()
layers = [13, 16, 8, 4, 1]  # Arquitetura mais profunda
```

### 2. Teste de Learning Rates
```python
# Teste diferentes taxas
mlp = MLP(layers, learning_rate=0.01)  # Taxa menor
mlp = MLP(layers, learning_rate=0.5)    # Taxa maior
```

### 3. Teste de FunÃ§Ãµes de AtivaÃ§Ã£o
```python
# Modifique em Node.activate() para testar ReLU ou tanh
def activate(self, x: float) -> float:
    return max(0, x)  # ReLU
```

## ğŸ“ˆ Resultados Esperados

### Dataset Heart Disease
- **Fonte**: Kaggle (johnsmith88/heart-disease-dataset)
- **Amostras**: 1,025 casos reais de pacientes
- **Features**: 13 caracterÃ­sticas clÃ­nicas
- **Classes**: 499 casos sem doenÃ§a, 526 com doenÃ§a cardÃ­aca
- **Accuracy**: 80-90% (testado: 81.46%)
- **ConvergÃªncia**: 300-500 Ã©pocas
- **Loss final**: 0.1-0.2

## ğŸ¯ PrÃ³ximos Passos

1. **Experimente** diferentes configuraÃ§Ãµes
2. **Analise** as visualizaÃ§Ãµes em tempo real
3. **Compare** diferentes arquiteturas
4. **Implemente** novas funÃ§Ãµes de ativaÃ§Ã£o
5. **Adicione** regularizaÃ§Ã£o (L1/L2)
6. **Teste** com outros datasets

## ğŸ’¡ Dicas de OtimizaÃ§Ã£o

- Use **learning rate decay** para melhor convergÃªncia
- Implemente **early stopping** para evitar overfitting
- Adicione **momentum** ao gradiente descendente
- Experimente **batch normalization**
- Teste **dropout** para regularizaÃ§Ã£o

---

**Boa sorte com seus experimentos! ğŸš€**
